To run a locally hosted dashboard in my browser I can run:
minikube dashboard --port=63840 (Port can be any port specified)

------------------

A Pod is the smallest and simplest unit in the Kubernetes object model that you create or deploy. It represents one (or sometimes more) running container(s) in a cluster. 
In a simple web application, you might have one single pod: the web server. As traffic grows, you might deploy that same code to multiple pods to handle the increased load. Several pods, one codebase. 
In a more complex backend system, you might have several pods for the web server and several pods that handle video processing. Multiple pods, multiple codebases.

Pod Illustration

Pods are just wrappers around containers. You can think of it as a Docker container with a little extra Kubernetes magic. The container is the actual application, 
and the Pod is the Kubernetes abstraction that manages the container and the resources it needs to run.

------------------

First, download a copy of your deployment's YAML file and save it in your current directory:

kubectl get deployment synergychat-web -o yaml > web-deployment.yaml

Then open it in your text editor. There are 5 top-level fields in the file:

- apiVersion: apps/v1 - Specifies the version of the Kubernetes API you're using to create the object (e.g., apps/v1 for Deployments).
- kind: Deployment - Specifies the type of object you're configuring
- metadata - Metadata about the deployment, like when it was created, its name, and its ID
- spec - The desired state of the deployment. Most impactful edits, like how many replicas you want, will be made here.
- status - The current state of the deployment. You won't edit this directly, it's just for you to see what's going on with your deployment.
- Inside your editor, change the number of replicas to 3 and save the file. Notice that you're just editing a file on your machine! It won't yet have any effect on the deployment in your cluster.

To apply the changes, run:

kubectl apply -f web-deployment.yaml

You should get a warning that lets you know that you're missing the last-applied-configuration annotation. That's okay! we got that warning because we created this deployment the quick and dirty way, 
by using kubectl create deployment instead of creating a YAML file and using kubectl apply -f.

However, because we've now updated it with kubectl apply, the annotation is now there, and we won't get the warning again.

-------------------

Create a deployment file then run: 

kubectl apply -f <deployment yaml filename>

-------------------

- Thrashing Pods
One of the most common problems you'll run into when working with Kubernetes is Pods that keep crashing and restarting. This is called "thrashing" and it's usually caused by one of a few things:

The application recently had a bug introduced in the latest image version
The application is misconfigured and can't start properly
A dependency of the application is misconfigured and the application can't start properly
The application is trying to use too much memory and is being killed by Kubernetes

- What Is “CrashLoopBackoff”?
When a pod's status is CrashLoopBackoff, that means the container is crashing (the program is exiting with error code 1).

Because Kubernetes is all about building self-healing systems, it will automatically restart the container. However, each time it tries to restart the container, if it crashes again, it will wait longer and longer in between restarts. That's why it's called a "backoff".

To fix a thrashing pod, you need to find out why it's crashing.

-------------------

Get kist of pods: 
kubectl get pods

Get the logs for a specified pod:
kubectl logs <pod-name>

-------------------

When making changes to a .yaml file always apply changes:

kubectl apply -f <.yaml file> 

** with config maps it doesnt apply to the pod, it just creates and stores the config map in k8.

Real organizations store their Kubernetes configurations (like your ConfigMap) in YAML files and keep them in version control (like Git). This means:

Changes can be tracked
Multiple team members can review changes
You can roll back to previous versions if needed
Repeatability: Using YAML files means you can:

Set up the same environment in development, staging, and production
Quickly recover if something goes wrong
Share configurations with team members
GitOps: Many organizations use GitOps practices where:

All changes to infrastructure are made through git
CI/CD pipelines automatically run kubectl apply -f when changes are merged

------------------------

- Applying the Config Map

Open up your api-deployment.yaml file. We're going to add a few things to it. Under the containers section, add the following to the first (and only) entry:

env:
  - name: API_PORT
    valueFrom:
      configMapKeyRef:
        name: synergychat-api-configmap
        key: API_PORT

This tells Kubernetes to set the API_PORT environment variable to the value of the API_PORT key in the synergychat-api-configmap config map. Reference the official docs if you're confused about the structure of the yaml.

Next, apply the deployment. Hopefully, you remember the command for this by now.

Once it's applied, you should be able to take a look at the pods and see that a new API pod has been deployed and isn't crashing!

Let's forward the API pod's 8080 port to our local machine so we can test it out.

kubectl port-forward <pod-name> 8080:8080

Make sure it returns a 404 response when you hit the root:

curl http://localhost:8080

-------------------------

Config Maps Are Insecure
ConfigMaps are a great way to manage innocent environment variables in Kubernetes. Things like:

Ports
URLs of other services
Feature flags
Settings that change between environments, like DEBUG mode
However, they are not cryptographically secure. ConfigMaps aren't encrypted, and they can be accessed by anyone with access to the cluster.

If you need to store sensitive information, you should use Kubernetes Secrets or a third-party solution.

-------------------------

We can use the envFrom key instead of the env key to reference the entire config map and make it available to the pods in the deployment:

instead of this:

spec:
  replicas: 1
  selector:
    matchLabels:
      app: synergychat-api
  template:
    metadata:
      labels:
        app: synergychat-api
    spec:
      containers:
      - name: synergychat-api
        image: bootdotdev/synergychat-api:latest
        env: # reference from here down #
          - name: API_PORT
            valueFrom:
              configMapKeyRef: 
                name: synergychat-api-configmap
                key: API_PORT
          - name: THING_TWO
            valueFrom:
              configMapKeyRef:
                name: synergychat-api-congigmap
                key: THING_TWO
          - name: THING_THREE
            ...

we can use envFrom:

spec:
  replicas: 1
  selector:
    matchLabels:
      app: synergychat-crawler
  template:
    metadata:
      labels:
        app: synergychat-crawler
    spec:
      containers:
      - name: synergychat-crawler
        image: bootdotdev/synergychat-crawler:latest
        envFrom:
          - configMapRef:
              name: synergychat-crawler-configmap

----------------------

Services
We've spun up pods and connected to them individually, but that's frankly not super useful if we want to distribute real traffic across those pods. That's where services come in.

Services provide a stable endpoint for pods. They are an abstraction used to provide a stable endpoint and load balance traffic across a group of Pods. By "stable endpoint", 
I just mean that the service will always be available at a given URL, even if the pod is destroyed and recreated.

Create a file called web-service.yaml and add the following:

apiVersion: v1
kind: Service
metadata/name: web-service (we could call it anything, but this is a fine name)
spec/selector/app: I'm going to let you figure out what should be here. This is how the service knows which pods to route traffic to.
spec/ports: An array of port objects. You need one entry:
protocol: TCP (TCP will allow us to use HTTP)
port: 80 (this is the port that the service will listen on)
targetPort: 8080 (this is the port that the pods are listening on)
This creates a new service called web-service with a few properties:

It listens on port 80 for incoming traffic
It forwards that traffic to pods that are listening on their port 8080
Its controller will continuously scan for pods matching the app: synergychat-web label selector and automatically add them to its pool


Now, let's forward the service's port to our local machine so we can test it out.

kubectl port-forward service/web-service 8080:80

----------------------

Service Types
Take a look at the yaml that describes your web-service.

kubectl get svc web-service -o yaml

"svc" is a short-hand alias for "service", either will work in kubectl.

You should see a section that looks like this:

spec:
  clusterIP: 10.96.213.234
  ...
  type: ClusterIP

We didn't specify a service type! Why is this here? Well, it's because ClusterIP is the default service type.

The clusterIP is the IP address that the service is bound to on the internal Kubernetes network. Remember how we talked about how pods get their own internal, 
virtual IP address? Well, services can too! However, type: ClusterIP is just one type of service! There are several others, including:

- NodePort: Exposes the Service on each Node's IP at a static port.

- LoadBalancer: Creates an external load balancer in the current cloud environment (if supported, e.g. AWS, GCP, Azure) and assigns a fixed, external IP to the service.

- ExternalName: Maps the Service to the contents of the externalName field (for example, to the hostname api.foo.bar.example). 

The mapping configures your cluster's DNS server to return a CNAME record with that external hostname value. No proxying of any kind is set up.
The interesting thing about service types is that they typically build on top of each other. For example, 
a NodePort service is just a ClusterIP service with the added functionality of exposing the service on each node's IP at a static port (it still has an internal cluster IP).

A LoadBalancer service is just a NodePort service with the added functionality of creating an external load balancer in the current cloud environment (it still has an internal cluster IP and node port).

An ExternalName service is actually a bit different. All it does is a DNS-level redirect. You can use it to redirect traffic from one service to another.

-------------------------

When a request comes in:

Web service receives user request
Web service calls api-service using its internal DNS name (like api-service.default.svc.cluster.local)
API service processes request, maybe calls crawler-service for data
Data flows back up the chain

Why Separate These Components?:
Scalability: Each component can scale independently
Isolation: If crawler fails, web and API might still work
Maintenance: Can update one component without touching others

Service Discovery:
Kubernetes automatically creates internal DNS entries
Services find each other using consistent names
If a pod dies, service redirects traffic to healthy pods

--------------------

DNS
Now that we've configured the ingress to route the domains:

synchat.internal to the web-service
synchatapi.internal to the api-service
We need to configure our local machine to resolve those domains to the ingress load balancer. We won't be setting up global DNS so that anyone on the internet can access our app! 
We'll just be configuring our local machine to resolve those domains to the ingress load balancer.

To resolve hostnames to IP addresses locally you can edit /etc/hosts file as an administrator in both WSL and PowerShell

I added these lines:

127.0.0.1 synchat.internal
127.0.0.1 synchatapi.internal

to PowerShell: notepad C:\Windows\System32\drivers\etc\hosts

to WSL: vim /etc/hosts

---------------------


Great question! The "network resolution process" refers to how a request travels from your browser to the actual Kubernetes pod running your application.

Let's break down the steps in order:

- DNS (/etc/hosts): When you type synchat.internal in your browser, your computer needs to figure out what IP address that domain points to. It checks your /etc/hosts file (which we modified earlier) 
  and finds that it points to 127.0.0.1.

- IP address: Now your browser knows to send the request to 127.0.0.1 (localhost), which is where the minikube tunnel is listening.

- Ingress controller: The request reaches the ingress controller (like NGINX) running in your Kubernetes cluster. The ingress controller looks at the host header (synchat.internal) 
  and path to determine which service should receive this request.

- Service: The ingress routes your request to the appropriate Kubernetes service, which acts as a stable internal address and load balancer for your pods.

- Pod: Finally, the service forwards the request to one of the available pods running your application.

---------------------

In production, the process would look more like this:

- DNS (real DNS servers): When users type your domain (like example.com), their request goes to real DNS servers on the internet, which resolve to your cloud provider's load balancer IP address.

- IP address: Traffic reaches your cloud provider's load balancer IP.

- Ingress controller: The load balancer forwards traffic to your Kubernetes ingress controller.

- Service: The ingress routes to the appropriate service.

- Pod: The service routes to a pod.

--- The localhost and /etc/hosts approach we're using is just a development convenience that mimics production while letting you work locally. In production, you'd:

- Register a real domain name
- Configure DNS to point to your cloud load balancer
- Set up TLS certificates for HTTPS

---------------------

DNS stands for Domain Name System. It's essentially the internet's phone book that translates human-friendly domain names (like google.com) into IP addresses (like 142.250.190.78) 
that computers use to identify each other.

TLS stands for Transport Layer Security. It's a cryptographic protocol that provides secure communication over a computer network. When you see HTTPS instead of HTTP in your browser, 
that "S" means the connection is secured with TLS. This ensures that data exchanged between your browser and the website is encrypted and can't be easily intercepted or tampered with.

---------------------

- Ingress Types
You may have noticed that at the top of all our resources we have this in the yaml:

apiVersion: v1

This is the API version of the resource, and because those resources are core to Kubernetes, they're in the standard v1 API group.

However, ingress isn't a core Kubernetes resource, it's an extension of sorts. That's why it has:

apiVersion: networking.k8s.io/v1

You can think of the networking.k8s.io API group as a core extension. It's not third-party (it's on k8s.io for heaven's sake), but it's not part of the core Kubernetes API either.

-----------------------

Annotations
You probably noticed that we used a single annotation on our ingress resource:

  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /

This is a really common pattern in Kubernetes. The core Kubernetes API is intentionally kept small, but there are a lot of things that people want to do with Kubernetes that aren't part of the core API. 
So, instead of adding a bunch of new fields to the core API, Kubernetes allows you to add arbitrary annotations to your resources, and then various extensions can read those annotations and do things with them.

For example, the Boot.dev Kubernetes cluster uses an ingress extension specific to Google Cloud Platform. We use the following annotations so that our controller knows which static IP address to use, 
which SSL certificate to use, and how to route traffic to our ingress:

  annotations:
    kubernetes.io/ingress.global-static-ip-name: static-ip-name-here
    networking.gke.io/managed-certificates: cert-name-here
    kubernetes.io/ingress.class: gce

If you're curious about the specifics, the docs are here. In a nutshell, however, the important take-away is that in most production deployments you'll be using annotations specific to the cloud provider you're using. 
Each major cloud provider has their own products, so you need to use k8s annotations and extensions specific to that cloud provider.

Now that you understand the basic concepts of ingress, in the future, it's just a matter of following the documentation for your cloud provider to get it set up.


When you see https instead of http it means your using TLS which a security protocol that created an encrypted connection between the web server and the browser.
SSL = Secure Sockets Layer
TLS = Transport Layer Security

-------------------------

Ephemeral Volumes
On-disk files in a container are ephemeral as we saw in the last lesson. This presents some problems for applications that want to save long-lived data across restarts. For example, user data in a database.

The Kubernetes volume abstraction solves two primary problems:

Data persistence
Data sharing across containers
As it turns out, there are a lot of different types of "volumes" in Kubernetes. Some are even ephemeral as well, just like a container's standard filesystem. 
The primary reason for using an ephemeral volume is to share data between containers in a pod.

Add a volumes section to spec/template/spec.
volumes:
  - name: cache-volume
    emptyDir: {}

Add a new volumeMounts section to the container entry. This will mount the volume we just created at the /cache path.
volumeMounts:
  - name: cache-volume
    mountPath: /cache

Duplicate the entire first entry in the containers list twice (you should now have 3 total containers). Update the name of each:
synergychat-crawler-1
synergychat-crawler-2
synergychat-crawler-3
Now all the containers in the pod will share the same volume at /cache. It's just an empty directory, but the crawler will use it to store its data.

Add a CRAWLER_DB_PATH environment variable to the crawler's ConfigMap. Set it to /cache/db. The crawler will use a directory called db inside the volume to store its data.
Apply the new ConfigMap and Deployment, and use kubectl get pod to see the status of your new pod.
You should notice that there's a problem with the pod! Only 1/3 of containers should be "ready". Use the logs command to get the logs for all 3 containers:

kubectl logs <podname> --all-containers

You should see something like this:

listen tcp :8080: bind: address already in use

Because pods share the same network namespace, they can't all bind to the same port! Hmm... let's put a bandaid on this by binding each container to a different port. 8080 is the only one that will 
be exposed via the service, but that's okay for now. We can add redundancy later.

Add two new values to the crawler's ConfigMap:

CRAWLER_PORT_2: 8081

CRAWLER_PORT_3: 8082

Update the crawler deployment:

Change the second and third containers to map CRAWLER_PORT_2 -> CRAWLER_PORT and CRAWLER_PORT_3 -> CRAWLER_PORT respectively (the Docker image expects a variable named "CRAWLER_PORT"). 
I'm not going to give you the code, but know that it's gonna be a bit tedious because you need to use 
env: instead of envFrom: for the second and third containers. Don't forget to continue exposing the CRAWLER_KEYWORDS and CRAWLER_DB_PATH environment variables for all containers.

-------------------------

-- Persistence

All the volumes we've worked with so far have been ephemeral, meaning when the associated pod is deleted the volume is deleted as well. This is fine for some use cases, 
but for most CRUD apps we want to persist data even if the pod is deleted.

If you think about it, it's not even just when pods are explicitly deleted with kubectl that we need to worry about data loss. Pods can be deleted for several reasons:

The node they're running on could fail
A new version of the image was published (code was updated, etc)
A new node was added to the cluster and the pod was rescheduled
In all of these cases, we want to make sure that our data is still available. Persistent volumes allow us to do this.

-- Persistent Volumes (PV)
Instead of simply adding a volume to a deployment, a persistent volume is a cluster-level resource that is created separately from the pod and then attached to the pod. It's similar to a ConfigMap in that way.

PVs can be created statically or dynamically.

Static PVs are created manually by a cluster admin
Dynamic PVs are created automatically when a pod requests a volume that doesn't exist yet
Generally speaking, and especially in the cloud-native world, we want to use dynamic PVs. It's less work and more flexible.

-- Persistent Volume Claims (PVC)
A persistent volume claim is a request for a persistent volume. When using dynamic provisioning, a PVC will automatically create a PV if one doesn't exist that matches the claim.

The PVC is then attached to a pod, just like a volume would be.

-- Assignment
Create a new file called api-pvc.yaml and add the following:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: xxx
spec:
  accessModes:
    - xxx
  resources:
    requests:
      storage: xxx

-- Add the following properties:

metadata/name: synergychat-api-pvc
spec/accessModes: An array with one entry:
ReadWriteOnce
spec/resources/requests/storage: 1Gi
This creates a new PVC called synergychat-api-pvc with a few properties that can be read from and written to by multiple pods at the same time. It also requests 1GB of storage.

Apply the PVC.

kubectl apply -f api-pvc.yaml

Run both of these commands:

kubectl get pvc
kubectl get pv

You should see that a new PV was created automatically!

-------------------------

-- Attach Persistence
So far all we've done is create an empty persistent volume. Let's get the api application to use it.

Assignment
Use your crawler deployment and the docs as a reference. Create a new volume in the api-deployment referencing your pvc:

volumes:
  - name: synergychat-api-volume
    persistentVolumeClaim:
      claimName: synergychat-api-pvc

Then mount it in the container under the /persist directory:

volumeMounts:
  - name: synergychat-api-volume
    mountPath: /persist

Update the API_DB_FILEPATH environment variable you added earlier to instead use the new mount path: /persist/db.json

Apply the changes, then check to make sure all your pods are healthy:

kubectl get pods

With your tunnel running (minikube tunnel -c), open http://synchat.internal/ in your browser.

Send some messages.
Delete the api pod.
Once the new pod is running, refresh the page and make sure your messages are still there. If they are, your persistent volume is working!

-------------------------

-- Databases
Now that you know all about volumes, you might be thinking, "Awesome! I'll host my CRUD app on Kubernetes and use a volume to store my PostgreSQL database data!".

That's certainly possible, but frankly, it's not always the best idea. For example, the Boot.dev system that powers this website has the following components:

A web application, currently served by Cloudflare (this could easily be a Kubernetes deployment if we cared to move it)
Several backend microservices, all running on Kubernetes in Google Cloud
Our main JSON CRUD API
A Discord bot
A service that compiles student's Go code to WASM
etc
A managed PostgreSQL database, hosted by Cloud SQL (GCP)
Why do we do this? Well, Kubernetes isn't always the simplest way to get a job done. We could certainly host a PostgreSQL database on Kubernetes, but it would require a lot of extra work to get it to work well. 
For example, we'd need to manually build all the configurations to:

-- Create a persistent volume
Handle Postgres version updates
Set resource limits
Set up automated backups
For that reason, when I need an SQL database, I typically use a managed service like Cloud SQL or RDS. There are exceptions to that rule, but it's a good rule of thumb.

-- When Would You Use a Database on Kubernetes?
I have used databases on Kubernetes in the past, but I've usually done it when the deployment wasn't exactly mission-critical. For example, I've deployed Grafana and Prometheus on Kubernetes, 
and they both have out-of-the-box support for in-cluster databases. I didn't care too much about backups and automatic upgrades for my telemetry data, and I knew the data set was small and static, so it was a good fit

-------------------------

Namespaces
To quote the Zen of Python:

Namespaces are one honking great idea -- let's do more of those!

Namespaces are a way to isolate cluster resources into groups. They're a bit like directories on your computer, but instead of containing files, they contain Kubernetes objects. 
As you've already learned, every resource in Kubernetes has a name. Some of our names include:

synergychat-api-configmap
api-service
api-deployment
web-deployment
...
You can only use a name once. It is a unique identifier. That's how kubectl apply knows when it should create a new resource and when it should update an existing one. 
Namespaces allow us to use the same name for different resources, as long as they're in different namespaces.

Run the following command:

kubectl get namespaces

or the shorter version:

kubectl get ns

-------------------------

Use this to restart a deployment after updating and appying a config map:
kubectl rollout restart deployment <deployment name> Use <kubectl get deployment> to get deployment name. 

minikube tunnel -c
minikube dashboard --port=63840

Kubernetes is an orchestration system that manages containerized applications, typically running in Docker containers (though it can work with other container runtimes too).

In this scenario:

You have multiple pods running in your Kubernetes cluster
Each pod contains one or more containers (which are typically Docker containers)
The containers run your actual application code
So rather than everything running in one Docker container, it's:

Multiple Docker containers
Grouped into pods (some pods have 1 container, the crawler pod has 3)
All managed by Kubernetes
This is more like a nested structure:

Kubernetes cluster contains many pods
Pods contain one or more containers
Containers run your applications

----------------------------