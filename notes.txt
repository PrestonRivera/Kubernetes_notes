To run a locally hosted dashboard in my browser I can run:
minikube dashboard --port=63840 (Port can be any port specified)

------------------

A Pod is the smallest and simplest unit in the Kubernetes object model that you create or deploy. It represents one (or sometimes more) running container(s) in a cluster. 
In a simple web application, you might have one single pod: the web server. As traffic grows, you might deploy that same code to multiple pods to handle the increased load. Several pods, one codebase. 
In a more complex backend system, you might have several pods for the web server and several pods that handle video processing. Multiple pods, multiple codebases.

Pod Illustration

Pods are just wrappers around containers. You can think of it as a Docker container with a little extra Kubernetes magic. The container is the actual application, 
and the Pod is the Kubernetes abstraction that manages the container and the resources it needs to run.

------------------

First, download a copy of your deployment's YAML file and save it in your current directory:

kubectl get deployment synergychat-web -o yaml > web-deployment.yaml

Then open it in your text editor. There are 5 top-level fields in the file:

- apiVersion: apps/v1 - Specifies the version of the Kubernetes API you're using to create the object (e.g., apps/v1 for Deployments).
- kind: Deployment - Specifies the type of object you're configuring
- metadata - Metadata about the deployment, like when it was created, its name, and its ID
- spec - The desired state of the deployment. Most impactful edits, like how many replicas you want, will be made here.
- status - The current state of the deployment. You won't edit this directly, it's just for you to see what's going on with your deployment.
- Inside your editor, change the number of replicas to 3 and save the file. Notice that you're just editing a file on your machine! It won't yet have any effect on the deployment in your cluster.

To apply the changes, run:

kubectl apply -f web-deployment.yaml

You should get a warning that lets you know that you're missing the last-applied-configuration annotation. That's okay! we got that warning because we created this deployment the quick and dirty way, 
by using kubectl create deployment instead of creating a YAML file and using kubectl apply -f.

However, because we've now updated it with kubectl apply, the annotation is now there, and we won't get the warning again.

-------------------

Create a deployment file then run: 

kubectl apply -f <deployment yaml filename>

-------------------

- Thrashing Pods
One of the most common problems you'll run into when working with Kubernetes is Pods that keep crashing and restarting. This is called "thrashing" and it's usually caused by one of a few things:

The application recently had a bug introduced in the latest image version
The application is misconfigured and can't start properly
A dependency of the application is misconfigured and the application can't start properly
The application is trying to use too much memory and is being killed by Kubernetes

- What Is “CrashLoopBackoff”?
When a pod's status is CrashLoopBackoff, that means the container is crashing (the program is exiting with error code 1).

Because Kubernetes is all about building self-healing systems, it will automatically restart the container. However, each time it tries to restart the container, if it crashes again, it will wait longer and longer in between restarts. That's why it's called a "backoff".

To fix a thrashing pod, you need to find out why it's crashing.

-------------------

Get kist of pods: 
kubectl get pods

Get the logs for a specified pod:
kubectl logs <pod-name>

-------------------

When making changes to a .yaml file always apply changes:

kubectl apply -f <.yaml file> 

** with config maps it doesnt apply to the pod, it just creates and stores the config map in k8.

Real organizations store their Kubernetes configurations (like your ConfigMap) in YAML files and keep them in version control (like Git). This means:

Changes can be tracked
Multiple team members can review changes
You can roll back to previous versions if needed
Repeatability: Using YAML files means you can:

Set up the same environment in development, staging, and production
Quickly recover if something goes wrong
Share configurations with team members
GitOps: Many organizations use GitOps practices where:

All changes to infrastructure are made through git
CI/CD pipelines automatically run kubectl apply -f when changes are merged

------------------------

- Applying the Config Map

Open up your api-deployment.yaml file. We're going to add a few things to it. Under the containers section, add the following to the first (and only) entry:

env:
  - name: API_PORT
    valueFrom:
      configMapKeyRef:
        name: synergychat-api-configmap
        key: API_PORT

This tells Kubernetes to set the API_PORT environment variable to the value of the API_PORT key in the synergychat-api-configmap config map. Reference the official docs if you're confused about the structure of the yaml.

Next, apply the deployment. Hopefully, you remember the command for this by now.

Once it's applied, you should be able to take a look at the pods and see that a new API pod has been deployed and isn't crashing!

Let's forward the API pod's 8080 port to our local machine so we can test it out.

kubectl port-forward <pod-name> 8080:8080

Make sure it returns a 404 response when you hit the root:

curl http://localhost:8080

-------------------------

Config Maps Are Insecure
ConfigMaps are a great way to manage innocent environment variables in Kubernetes. Things like:

Ports
URLs of other services
Feature flags
Settings that change between environments, like DEBUG mode
However, they are not cryptographically secure. ConfigMaps aren't encrypted, and they can be accessed by anyone with access to the cluster.

If you need to store sensitive information, you should use Kubernetes Secrets or a third-party solution.

-------------------------

We can use the envFrom key instead of the env key to reference the entire config map and make it available to the pods in the deployment:

instead of this:

spec:
  replicas: 1
  selector:
    matchLabels:
      app: synergychat-api
  template:
    metadata:
      labels:
        app: synergychat-api
    spec:
      containers:
      - name: synergychat-api
        image: bootdotdev/synergychat-api:latest
        env: # reference from here down #
          - name: API_PORT
            valueFrom:
              configMapKeyRef: 
                name: synergychat-api-configmap
                key: API_PORT
          - name: THING_TWO
            valueFrom:
              configMapKeyRef:
                name: synergychat-api-congigmap
                key: THING_TWO
          - name: THING_THREE
            ...

we can use envFrom:

spec:
  replicas: 1
  selector:
    matchLabels:
      app: synergychat-crawler
  template:
    metadata:
      labels:
        app: synergychat-crawler
    spec:
      containers:
      - name: synergychat-crawler
        image: bootdotdev/synergychat-crawler:latest
        envFrom:
          - configMapRef:
              name: synergychat-crawler-configmap

----------------------

Services
We've spun up pods and connected to them individually, but that's frankly not super useful if we want to distribute real traffic across those pods. That's where services come in.

Services provide a stable endpoint for pods. They are an abstraction used to provide a stable endpoint and load balance traffic across a group of Pods. By "stable endpoint", 
I just mean that the service will always be available at a given URL, even if the pod is destroyed and recreated.

Create a file called web-service.yaml and add the following:

apiVersion: v1
kind: Service
metadata/name: web-service (we could call it anything, but this is a fine name)
spec/selector/app: I'm going to let you figure out what should be here. This is how the service knows which pods to route traffic to.
spec/ports: An array of port objects. You need one entry:
protocol: TCP (TCP will allow us to use HTTP)
port: 80 (this is the port that the service will listen on)
targetPort: 8080 (this is the port that the pods are listening on)
This creates a new service called web-service with a few properties:

It listens on port 80 for incoming traffic
It forwards that traffic to pods that are listening on their port 8080
Its controller will continuously scan for pods matching the app: synergychat-web label selector and automatically add them to its pool


Now, let's forward the service's port to our local machine so we can test it out.

kubectl port-forward service/web-service 8080:80

----------------------

Service Types
Take a look at the yaml that describes your web-service.

kubectl get svc web-service -o yaml

"svc" is a short-hand alias for "service", either will work in kubectl.

You should see a section that looks like this:

spec:
  clusterIP: 10.96.213.234
  ...
  type: ClusterIP

We didn't specify a service type! Why is this here? Well, it's because ClusterIP is the default service type.

The clusterIP is the IP address that the service is bound to on the internal Kubernetes network. Remember how we talked about how pods get their own internal, 
virtual IP address? Well, services can too! However, type: ClusterIP is just one type of service! There are several others, including:

- NodePort: Exposes the Service on each Node's IP at a static port.

- LoadBalancer: Creates an external load balancer in the current cloud environment (if supported, e.g. AWS, GCP, Azure) and assigns a fixed, external IP to the service.

- ExternalName: Maps the Service to the contents of the externalName field (for example, to the hostname api.foo.bar.example). 

The mapping configures your cluster's DNS server to return a CNAME record with that external hostname value. No proxying of any kind is set up.
The interesting thing about service types is that they typically build on top of each other. For example, 
a NodePort service is just a ClusterIP service with the added functionality of exposing the service on each node's IP at a static port (it still has an internal cluster IP).

A LoadBalancer service is just a NodePort service with the added functionality of creating an external load balancer in the current cloud environment (it still has an internal cluster IP and node port).

An ExternalName service is actually a bit different. All it does is a DNS-level redirect. You can use it to redirect traffic from one service to another.

-------------------------

When a request comes in:

Web service receives user request
Web service calls api-service using its internal DNS name (like api-service.default.svc.cluster.local)
API service processes request, maybe calls crawler-service for data
Data flows back up the chain

Why Separate These Components?:
Scalability: Each component can scale independently
Isolation: If crawler fails, web and API might still work
Maintenance: Can update one component without touching others

Service Discovery:
Kubernetes automatically creates internal DNS entries
Services find each other using consistent names
If a pod dies, service redirects traffic to healthy pods

--------------------

DNS
Now that we've configured the ingress to route the domains:

synchat.internal to the web-service
synchatapi.internal to the api-service
We need to configure our local machine to resolve those domains to the ingress load balancer. We won't be setting up global DNS so that anyone on the internet can access our app! 
We'll just be configuring our local machine to resolve those domains to the ingress load balancer.

To resolve hostnames to IP addresses locally you can edit /etc/hosts file as an administrator in both WSL and PowerShell

I added these lines:

127.0.0.1 synchat.internal
127.0.0.1 synchatapi.internal

to PowerShell: notepad C:\Windows\System32\drivers\etc\hosts

to WSL: vim /etc/hosts

---------------------


Great question! The "network resolution process" refers to how a request travels from your browser to the actual Kubernetes pod running your application.

Let's break down the steps in order:

- DNS (/etc/hosts): When you type synchat.internal in your browser, your computer needs to figure out what IP address that domain points to. It checks your /etc/hosts file (which we modified earlier) 
  and finds that it points to 127.0.0.1.

- IP address: Now your browser knows to send the request to 127.0.0.1 (localhost), which is where the minikube tunnel is listening.

- Ingress controller: The request reaches the ingress controller (like NGINX) running in your Kubernetes cluster. The ingress controller looks at the host header (synchat.internal) 
  and path to determine which service should receive this request.

- Service: The ingress routes your request to the appropriate Kubernetes service, which acts as a stable internal address and load balancer for your pods.

- Pod: Finally, the service forwards the request to one of the available pods running your application.

---------------------

In production, the process would look more like this:

- DNS (real DNS servers): When users type your domain (like example.com), their request goes to real DNS servers on the internet, which resolve to your cloud provider's load balancer IP address.

- IP address: Traffic reaches your cloud provider's load balancer IP.

- Ingress controller: The load balancer forwards traffic to your Kubernetes ingress controller.

- Service: The ingress routes to the appropriate service.

- Pod: The service routes to a pod.

--- The localhost and /etc/hosts approach we're using is just a development convenience that mimics production while letting you work locally. In production, you'd:

- Register a real domain name
- Configure DNS to point to your cloud load balancer
- Set up TLS certificates for HTTPS

---------------------

DNS stands for Domain Name System. It's essentially the internet's phone book that translates human-friendly domain names (like google.com) into IP addresses (like 142.250.190.78) 
that computers use to identify each other.

TLS stands for Transport Layer Security. It's a cryptographic protocol that provides secure communication over a computer network. When you see HTTPS instead of HTTP in your browser, 
that "S" means the connection is secured with TLS. This ensures that data exchanged between your browser and the website is encrypted and can't be easily intercepted or tampered with.

---------------------

- Ingress Types
You may have noticed that at the top of all our resources we have this in the yaml:

apiVersion: v1

This is the API version of the resource, and because those resources are core to Kubernetes, they're in the standard v1 API group.

However, ingress isn't a core Kubernetes resource, it's an extension of sorts. That's why it has:

apiVersion: networking.k8s.io/v1

You can think of the networking.k8s.io API group as a core extension. It's not third-party (it's on k8s.io for heaven's sake), but it's not part of the core Kubernetes API either.

-----------------------

Annotations
You probably noticed that we used a single annotation on our ingress resource:

  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /

This is a really common pattern in Kubernetes. The core Kubernetes API is intentionally kept small, but there are a lot of things that people want to do with Kubernetes that aren't part of the core API. 
So, instead of adding a bunch of new fields to the core API, Kubernetes allows you to add arbitrary annotations to your resources, and then various extensions can read those annotations and do things with them.

For example, the Boot.dev Kubernetes cluster uses an ingress extension specific to Google Cloud Platform. We use the following annotations so that our controller knows which static IP address to use, 
which SSL certificate to use, and how to route traffic to our ingress:

  annotations:
    kubernetes.io/ingress.global-static-ip-name: static-ip-name-here
    networking.gke.io/managed-certificates: cert-name-here
    kubernetes.io/ingress.class: gce

If you're curious about the specifics, the docs are here. In a nutshell, however, the important take-away is that in most production deployments you'll be using annotations specific to the cloud provider you're using. 
Each major cloud provider has their own products, so you need to use k8s annotations and extensions specific to that cloud provider.

Now that you understand the basic concepts of ingress, in the future, it's just a matter of following the documentation for your cloud provider to get it set up.


When you see https instead of http it means your using TLS which a security protocol that created an encrypted connection between the web server and the browser.
SSL = Secure Sockets Layer
TLS = Transport Layer Security

-------------------------

Use this to restart a deployment after updating and appying a config map:
kubectl rollout restart deployment <deployment name> Use <kubectl get deployment> to get deployment name. 

minikube tunnel -c
minikube dashboard --port=63840
